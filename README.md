# Admin RAG - French Labor Law Assistant

Agentic RAG system for French labor law questions, combining Code du travail and KALI conventions (collective agreements).

**Example question**: "What is my pÃ©riode d'essai prÃ©avis if I have worked more than a year and I'm in informatics (Syntec convention)?"

## Quick Start

```bash
# 1. Clone repository
git clone https://github.com/psorianom/admin-rag.git
cd admin-rag

# 2. Install Poetry (if needed)
curl -sSL https://install.python-poetry.org | python3 -

# 3. Download raw data (see Data Sources below)
# Place in data/raw/

# 4. Run full pipeline
make all
```

That's it! The Makefile handles:
- âœ… Installing dependencies
- âœ… Starting Qdrant vector store
- âœ… Parsing XML â†’ structured JSONL
- âœ… Generating BGE-M3 embeddings
- âœ… Indexing into vector database

## Prerequisites

- **Python**: 3.10+
- **Docker**: For Qdrant vector store
- **Poetry**: Dependency management
- **Disk space**: ~10GB for raw data + processed outputs
- **RAM**: 4-6GB minimum (for embeddings)

## Data Sources

### Code du travail
Download from [data.gouv.fr LEGI dataset](https://www.data.gouv.fr/fr/datasets/legi-codes-lois-et-reglements-consolides/):
```bash
# Extract to data/raw/code_travail_LEGITEXT000006072050/
```

### KALI (Conventions collectives)
Download KALI corpus from [data.gouv.fr](https://www.data.gouv.fr/fr/datasets/kali-conventions-collectives-nationales/):
```bash
# Extract to data/raw/kali/kali/global/
```

## Project Structure

```
admin-rag/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/           # Original XML from Legifrance
â”‚   â”œâ”€â”€ processed/     # Cleaned, chunked JSONL documents
â”‚   â””â”€â”€ eval/          # Evaluation datasets
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/     # Data extraction & processing (Phase 1)
â”‚   â”œâ”€â”€ retrieval/     # RAG components (Phase 2)
â”‚   â”œâ”€â”€ agents/        # Agentic layer (Phase 3)
â”‚   â””â”€â”€ evaluation/    # Quality assessment (Phase 4)
â”œâ”€â”€ scripts/           # Utility scripts
â”œâ”€â”€ notebooks/         # Exploration & experiments
â”œâ”€â”€ configs/           # Configuration files
â”œâ”€â”€ tests/             # Unit tests
â”œâ”€â”€ Makefile           # Pipeline automation
â”œâ”€â”€ FLOW.md            # Development log & decisions
â””â”€â”€ TODO.md            # Project roadmap
```

## Makefile Targets

### Main Workflows

```bash
make all              # Run full pipeline (setup â†’ parse â†’ ingest)
make setup            # Install dependencies + start Qdrant
make parse            # Parse Code du travail + KALI (Phase 1)
make ingest           # Embed and index into Qdrant (Phase 2)
make ingest-only      # Ingest from existing JSONL files (skip parsing)
make status           # Check pipeline status
```

**If you already have parsed JSONL files (no raw data needed):**
```bash
# Place files in data/processed/:
#   - code_travail_chunks.jsonl (16MB)
#   - kali_chunks.jsonl (24MB)

make setup           # Install deps + start Qdrant
make ingest-only     # Embed and index (skips parsing)
```

This is ideal for:
- Moving between machines (40MB vs 10GB raw data)
- Cloud GPU instances (vast.ai) - just upload JSONL files
- Sharing with collaborators

### Individual Steps

```bash
make parse-code-travail    # Parse Code du travail only
make parse-kali            # Parse KALI conventions only
make ingest-code-travail   # Ingest Code du travail only
make ingest-kali           # Ingest KALI only (when implemented)
```

### Utilities

```bash
make start-qdrant     # Start Qdrant Docker container
make stop-qdrant      # Stop Qdrant
make clean            # Remove cache files
make clean-processed  # Remove processed JSONL files
make clean-qdrant     # Remove Qdrant storage (destructive!)
```

## Development Phases

### âœ… Phase 1: Data Pipeline (Complete)
- Parsed 41,815 Code du travail articles â†’ 11,644 current chunks
- Parsed 289,936 KALI articles â†’ 14,154 chunks (7 major conventions)
- Semantic chunking strategy preserving legal structure
- Rich metadata: hierarchy, section titles, dates, status

**Output**:
- `data/processed/code_travail_chunks.jsonl` (11,644 chunks)
- `data/processed/kali_chunks.jsonl` (14,154 chunks)

### ğŸš§ Phase 2: Retrieval Foundation (In Progress)
- **Vector Store**: Qdrant (Docker-based)
- **Embedding Model**: BGE-M3 (1024 dims, French-optimized)
- **Ingestion Pipelines**: Haystack 2.x with auto GPU/CPU detection
  - `ingest_code_travail.py` - 11,644 chunks
  - `ingest_kali.py` - 14,154 chunks from 7 conventions
- Separate collections for explicit agent routing
- Full automation via Makefile

**Status**: Ingestion scripts ready, awaiting execution (pending GPU/CPU decision)

### â³ Phase 3: Agentic Layer (Pending)
- Multi-step reasoning workflow
- Convention identification tool
- Dual-source retrieval (Code du travail + KALI)
- Rule comparison and synthesis

### â³ Phase 4: Evaluation & Quality (Pending)
- Test dataset creation
- Retrieval quality tuning
- Citation system
- Edge case handling

## Tech Stack

- **Language**: Python 3.10+
- **Dependency Management**: Poetry
- **RAG Framework**: Haystack 2.x
- **Vector Store**: Qdrant (Docker)
- **Embeddings**: BGE-M3 via sentence-transformers
- **Data Format**: XML â†’ JSONL
- **Automation**: Make

## Key Design Decisions

### Why separate collections (not merged)?
Agent needs explicit routing logic: "Check Code du travail first, then query specific convention". This enables side-by-side comparison where convention rules override base labor code.

### Why semantic chunking (not fixed windows)?
Preserves legal structure (articles, alinÃ©as). Matches how legal professionals cite law. More interpretable results.

### Why BGE-M3?
Validated by [AgentPublic/legi dataset](https://huggingface.co/datasets/AgentPublic/legi) on French legal text. Multilingual, good French performance, reasonable size (1024 dims).

### Why not use pre-processed AgentPublic dataset?
We want flexibility to experiment with chunking strategies. Their fixed-window approach (5000 chars + overlap) creates 2.9x more chunks but splits mid-paragraph.

## Documentation

- **FLOW.md**: Development log with implementation details and decisions
- **TODO.md**: Project roadmap and task breakdown
- **CLAUDE.md**: Instructions for Claude Code (coding assistant)

## Current Status

**Completed**:
- âœ… Data parsing pipeline (11,644 + 14,154 chunks)
- âœ… Qdrant setup and configuration
- âœ… Ingestion pipelines for both Code du travail and KALI
- âœ… Full automation via Makefile
- âœ… Comprehensive documentation

**Next Steps**:
- Run embeddings ingestion (pending GPU/CPU decision)
  - Option A: BGE-M3 on vast.ai GPU (~$0.10, 20 min)
  - Option B: Smaller model on CPU (free, 30 min)
- Implement basic retrieval pipeline
- Test retrieval quality with sample queries

## License

To be determined

## Contributing

This is a personal research project. See FLOW.md for development context.
